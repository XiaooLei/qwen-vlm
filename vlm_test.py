import torch
from PIL import Image
import os
from model import VLMModel  # ç¡®ä¿ä½ çš„ç±»å®šä¹‰åœ¨ model.py ä¸­
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "Qwen/Qwen2.5-0.5B"



device = "cuda" if torch.cuda.is_available() else "cpu"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map=device,
)




def encode_test():
    ids = tokenizer.encode("<image>", add_special_tokens=False)
    print(f"IDåºåˆ—: {ids}") 
# ç»“æœé€šå¸¸æ˜¯ï¼š[1350, 9631, 1352]  (å¯¹åº” '<', 'image', '>')


def run_test():
    # --- 1. é…ç½®å‚æ•° ---
    # æŒ‡å‘ä½ æ˜æ—©è·‘å‡ºæ¥çš„æœ€å¼ºæƒé‡
    checkpoint_path = "./checkpoints/projector_final_qwen2.5-0.5b-instruct_clip-vit-base-patch16.pt" 

    # /Users/admin/Desktop/workspace/ai/nlp-beginer/lab6-vlm/checkpoints/projector_final_qwen2.5-0.5b-instruct_clip-vit-base-patch16.pt
    # æµ‹è¯•å›¾ç‰‡è·¯å¾„
    test_image_path = "./llava_data/train2017/000000000081.jpg" 
    
    # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu")
    print(f"æ£€æµ‹åˆ°è®¾å¤‡: {device}")

    # --- 2. åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½æƒé‡ ---
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    # æ³¨æ„ï¼šåˆå§‹åŒ–æ—¶å…ˆä¸ä¼  projector_paramsï¼Œæ‰‹åŠ¨ load æ›´æ¸…æ™°
    model = VLMModel() 
    
    if os.path.exists(checkpoint_path):
        print(f"æ­£åœ¨åŠ è½½è®­ç»ƒæˆæœ: {checkpoint_path}")
        state_dict = torch.load(checkpoint_path, map_location=device)
        model.projector.load_state_dict(state_dict)
    else:
        print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ {checkpoint_path}ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„ Projector è¿›è¡Œæµ‹è¯•ã€‚")

    model.to(device)
    model.eval()

    # --- 3. å‡†å¤‡æµ‹è¯•å›¾ç‰‡ ---
    if not os.path.exists(test_image_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æµ‹è¯•å›¾ç‰‡ {test_image_path}")
        return

    image = Image.open(test_image_path).convert("RGB")
    print(f"æˆåŠŸåŠ è½½å›¾ç‰‡: {test_image_path}")

    # --- 4. å¼€å§‹æé—® ---
    test_questions = [
        "What is in this image?",
    ]

    print("\n" + "="*30)
    print("ğŸš€ VLM æ¨ç†æµ‹è¯•å¼€å§‹")
    print("="*30)

    for i, q in enumerate(test_questions):
        print(f"\n[é—®é¢˜ {i+1}]: {q}")
        # ç›´æ¥è°ƒç”¨ä½ é›†æˆåœ¨ç±»é‡Œçš„ answer æ–¹æ³•
        response = model.answer(image, q, max_new_tokens=128)
        print(f"AI å›å¤: {response}")

    print("\n" + "="*30)
    print("æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    run_test()